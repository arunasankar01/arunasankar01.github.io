---
layout: default
---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

</head>
<body>

<h2>Introduction / Background</h2>
<p>AI models have become prominent in recent developments but are often regarded as black-box systems, making it challenging to understand their decision-making processes [1]. This has led to a growing emphasis on interpretability, especially in high-stakes fields such as healthcare. Recent advancements in explainable AI (XAI), including Concept Bottleneck Models (CBMs) [2, 3], have sought to create associations between human-understandable concepts and model outputs, enhancing interpretability. This project aims to develop interpretable AI models that provide insights into the underlying factors influencing disease detection using the ChestX-ray8 dataset [4] and the HAM10000 dataset [5]. The latter is vital for dermatological research, showcasing deep learning's effectiveness in skin cancer classification and the need for interpretability in clinical settings.</p>

<h2>Dataset Description</h2>
<p>This project utilizes two medical imaging datasets: <a href="https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000" target="_blank">HAM10000</a> and <a href="https://nihcc.app.box.com/v/ChestXray-NIHCC" target="_blank">ChestX-ray8</a>. The HAM10000 dataset consists of 10,015 dermatoscopic images, each labeled with one of seven skin lesion conditions. These images will aid in developing an interpretable AI model that generates concepts based on visual features to explain skin condition predictions. The ChestX-ray8 dataset includes 108,948 frontal X-ray images annotated with 12 thoracic disease labels. A subset of 10,000 images will be used for training, with plans for future scaling. We will enhance interpretability by generating concepts related to lung opacity using a language model like GPT.</p>

<h2>Problem Definition</h2>
<p>Deep neural networks face two critical challenges in real-world applications: the inability to learn continuously without forgetting past knowledge and the lack of interpretability in their decision-making processes. While there has been progress in continual learning and model interpretability, the intersection of these areas remains underexplored. Our goal is to develop models that learn new information over time while retaining past knowledge and providing human-understandable, text-based explanations. This will lead to adaptable and transparent models, which are essential in healthcare and autonomous systems where trust and safety are paramount.</p>

<h2>Motivation</h2>
<p>The black-box nature of neural network models motivates this project. As AI systems in healthcare and autonomous fields become more prevalent, understanding these models is crucial. Trust in AI relies on clear explanations for decisions, especially in medicine, where professionals depend on AI for accurate diagnoses. Additionally, AI models must adapt over time to enhance reliability. Our ultimate goal is to develop AI systems that foster meaningful insights and collaboration between machines and humans in critical settings.</p>

<h2>Methods</h2>
<h3>Data Preprocessing</h3>
<p>The data preprocessing involved the following steps:</p>
<ul>
    <li><strong>Data Collection:</strong> The HAM-10000 dataset was downloaded and organized into seven separate folders, each representing a specific class based on the image-to-class mapping provided in the CSV file.</li>
    <li><strong>Initial Data Analysis:</strong> An analysis of the dataset showed it was imbalanced, with the following label distribution:</li>
</ul>

<table>
    <table>
        <thead>
            <tr>
                <th>Class Label</th>
                <th>Initial Count</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>akiec</td>
                <td>327</td>
            </tr>
            <tr>
                <td>bcc</td>
                <td>514</td>
            </tr>
            <tr>
                <td>bkl</td>
                <td>1099</td>
            </tr>
            <tr>
                <td>df</td>
                <td>115</td>
            </tr>
            <tr>
                <td>mel</td>
                <td>1113</td>
            </tr>
            <tr>
                <td>nv</td>
                <td>6705</td>
            </tr>
            <tr>
                <td>vasc</td>
                <td>142</td>
            </tr>
        </tbody>
    </table>

<ul>
    <li><strong>Data Augmentation:</strong> To address the class imbalance, augmentation techniques such as cropping, rotation, and vertical flipping were applied. This process ensured that each class contained 500 images, resulting in a total of 3,500 images. Due to compute limitations, the model was run on this set of 3,500 images.</li>
    <li><strong>Data Splitting:</strong> The augmented dataset was divided into training, validation, and test sets using a 70/20/10 split and stored in .p files.</li>
    <li><strong>Concept Generation:</strong> For concept generation, 25 images from each class were processed through the GPT-4.0 model, producing 50 distinct concepts per class. These concepts were stored in a JSON file and split into 35 for training and 15 for testing.</li>
</ul>
<h3>ML Algorithms/Models Implemented</h3>

    <h3>Unsupervised Learning</h3>
    
    <h4>K-Means Clustering</h4>
    <p>The K-Means algorithm is applied with multiple values of <code>n_clusters</code>, starting from 2, 4, 6, and 7. For each value of <code>K</code>, the algorithm attempts to classify the data into that number of clusters, which may correspond to different skin cancer types. After fitting the model for each <code>K</code>, cluster assignments for each image are stored. This process can be viewed as an attempt to simulate continual learning in clustering, where the model adapts and evolves by increasing the number of clusters progressively. The average precision scores for each <code>K</code> value are calculated to assess the purity of clusters at each step, providing insight into how the clustering evolves with each increase in <code>K</code>.</p>
    <h4>Clustering Results with Forgetting</h4>
    <table>
        <tr>
            <th>K (Number of Clusters)</th>
            <th>Average Cluster Precision</th>
            <th>Forgetting</th>
        </tr>
        <tr>
            <td>2</td>
            <td>0.6691</td>
            <td>-</td>
        </tr>
        <tr>
            <td>4</td>
            <td>0.6450</td>
            <td>0.0241</td>
        </tr>
        <tr>
            <td>6</td>
            <td>0.6448</td>
            <td>0.0002</td>
        </tr>
        <tr>
            <td>7</td>
            <td>0.6283</td>
            <td>0.0165</td>
        </tr>
    </table>
    
    <h3>Supervised Learning</h3>
    
    <h4>Concept-Guided Visual Classifier (CGVC)</h4> 
    <p>Our CGVC model uses advanced AI language tools to automatically create easy-to-understand concepts for image classification, without needing humans to manually label data. Here's how it works:</p> <ol> <li><strong>Concept Creation:</strong> We use ChatGPT 4.0 to generate a list of potential concepts for each image class.</li> <li><strong>Concept Selection:</strong> We choose the best concepts using a smart selection method that: <ul> <li>Picks concepts that are unique to specific classes</li> <li>Ensures a diverse range of concepts for each class</li> </ul> </li> <li><strong>Concept-Image Matching:</strong> We use CLIP, an AI tool, to compare how well each concept matches with the images.</li> <li><strong>Classification:</strong> We use a simple math equation to predict the image class based on how well it matches with different concepts.</li> <li><strong>Training:</strong> We teach the model using 3500 images (500 per class) to fine-tune its predictions.</li> </ol> <p>The CGVC model helps us understand why it makes certain predictions by showing which concepts it thinks are important for each image. This makes the model's decision-making process clearer while still being as accurate as other advanced methods.</p>

<h3>Concept Annotation</h3>
<ul>
    <li><strong>Concept Generation:</strong> Use a GPT model to generate concise captions for selected images.</li>
    <li><strong>Text Cleaning:</strong> Refine generated concepts to maintain quality.</li>
    <li><strong>Annotation Expansion:</strong> Apply generated captions across all images in the class.</li>
</ul>

<h2>Results and Discussion</h2>
<h3>Quantitative Metrics</h3>
<ul>
    <li><strong>Final Average Accuracy (FAA):</strong> Measures overall classification accuracy after all tasks.</li>
    <li><strong>Average Forgetting (AF):</strong> Indicates how much the model forgets earlier tasks when learning new ones.</li>
    <li><strong>Active Concept Ratio (ACR):</strong> Tracks the utility of concepts across tasks, providing insight into knowledge retention.</li>
    <li><strong>Concept Credibility (CC):</strong> Evaluates the validity of concepts learned by the model.</li>
</ul>

<h2>Project Goals</h2>
<ul>
    <li><strong>Develop Continuous Learning:</strong> Create a neural network model capable of learning new information without forgetting previously acquired knowledge.</li>
    <li><strong>Enhance Interpretability:</strong> Provide clear, human-understandable insights into decision-making processes.</li>
</ul>

<h2>Expected Results</h2>
<p>We anticipate high diagnostic accuracy for the AI models developed from the HAM10000 and ChestX-ray8 datasets, measured by FAA. Models should exhibit low AF, demonstrating their ability to learn new information while retaining past knowledge. Enhanced interpretability will be assessed through the ACR metric, indicating effective concept utilization. Ultimately, we expect our model to adapt over time, improving performance in healthcare applications.</p>

<h2>Gantt Chart</h2>
<img src="ML Team55 Gantt Chart.jpg" alt="Gantt Chart">

<h2>Contribution Table</h2>
<table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Proposal Contributions</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Ajay Bhat</td>
        <td>Introduction & Background, Potential Results & Discussion</td>
      </tr>
      <tr>
        <td>Aruneswari Sankar</td>
        <td>Potential Results & Discussion, Github Page</td>
      </tr>
      <tr>
        <td>Saikrishnan Sankar</td>
        <td>Methods, Github Page</td>
      </tr>
      <tr>
        <td>Sri Siddarth Chakaravarthy P</td>
        <td>Introduction & Background, Methods</td>
      </tr>
      <tr>
        <td>Vimalan K Manivannan</td>
        <td>Introduction & Background, Problem Definition</td>
      </tr>
    </tbody>
  </table>

<h2>Proposal Video & Presentation Slides</h2>
<p><a href="https://youtu.be/BnxMtnb5css" target="_blank">Proposal Video</a></p>
<p><a href="https://docs.google.com/presentation/d/1gqot6oGNl0aXS3QraKKgu5Ru6e4wC4C3BTgFA2KYU7E/edit?usp=sharing" target="_blank">Slides</a></p>

<h2>References</h2>
<ol>
    <li>Rudin, C. (2019). <em>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.</em> Nature Machine Intelligence, 1(5), 206-215.</li>
    <li>Koh, P. W., et al. (2020). <em>Concept bottleneck models.</em> In International conference on machine learning (pp. 5338-5348). PMLR.</li>
    <li>Yang, Y., et al. (2023). <em>Language in a bottle: Language model guided concept bottlenecks for interpretable image classification.</em> In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 19187-19197).</li>
    <li>Wang X, Peng Y, Lu L, et al. (2017). <em>ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.</em> CVPR 2017. <a href="https://nihcc.app.box.com/v/ChestXray-NIHCC">https://nihcc.app.box.com/v/ChestXray-NIHCC</a></li>
    <li>Esteva A, Kuprel B, et al. (2017). <em>Dermatologist-level classification of skin cancer with deep neural networks.</em> Nature, 542(7639), 115-118. <a href="https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000">https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000</a></li>
</ol>




</body>
</html>
